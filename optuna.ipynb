{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Hyperparameter tuning with Optuna\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn the importance of tuning hyperparameters. You will first try to optimize the parameters manually and then we will see how to automate the search using Optuna.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "```bash\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYdv2ygjLaFL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3 sb3-contrib optuna --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R7tKaBFrTR0a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN\n",
        "from sb3_contrib import QRDQN, TQC\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwFOp0j-ga-_"
      },
      "source": [
        "# Automatic Hyperparameter Tuning\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88x7wMyyud5p"
      },
      "source": [
        "In this part we will create a script that allows to search for the best hyperparameters automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auwR-30IvHeY"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VM6tUr-yuekR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVfmM1dzA1d"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yyBTVcAGzCRk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "N_TRIALS = 100  # Maximum number of trials\n",
        "N_JOBS = 1 # Number of jobs to run in parallel\n",
        "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
        "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
        "N_TIMESTEPS = int(2e4)  # Training budget\n",
        "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
        "N_EVAL_ENVS = 5\n",
        "N_EVAL_EPISODES = 10\n",
        "TIMEOUT = int(60 * 15)  # 15 minutes\n",
        "\n",
        "ENV_ID = \"CartPole-v1\"\n",
        "\n",
        "DEFAULT_HYPERPARAMS = {\n",
        "    \"policy\": \"MlpPolicy\",\n",
        "    \"env\": ENV_ID,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25HgcDYzvJ0b"
      },
      "source": [
        "### Define the search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KXo8AwGAvN8Q",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Sampler for A2C hyperparameters.\n",
        "\n",
        "    :param trial: Optuna trial object\n",
        "    :return: The sampled hyperparameters for the given trial.\n",
        "    \"\"\"\n",
        "    # Discount factor between 0.9 and 0.9999\n",
        "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
        "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
        "    # 8, 16, 32, ... 1024\n",
        "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
        "\n",
        "\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\",1e-5, 1, log=True)\n",
        "    net_arch = trial.suggest_categorical(\"net_arch\",choices=[\"tiny\", \"small\"])\n",
        "    activation_fn = trial.suggest_categorical(\"activation_fn\",choices=[\"tanh\",\"relu\"])\n",
        "\n",
        "    # Display true values\n",
        "    trial.set_user_attr(\"gamma_\", gamma)\n",
        "    trial.set_user_attr(\"n_steps\", n_steps)\n",
        "\n",
        "    net_arch = [\n",
        "        {\"pi\": [64], \"vf\": [64]}\n",
        "        if net_arch == \"tiny\"\n",
        "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
        "    ]\n",
        "\n",
        "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
        "\n",
        "    return {\n",
        "        \"n_steps\": n_steps,\n",
        "        \"gamma\": gamma,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_grad_norm\": max_grad_norm,\n",
        "        \"policy_kwargs\": {\n",
        "            \"net_arch\": net_arch,\n",
        "            \"activation_fn\": activation_fn,\n",
        "        },\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iybymNiJxNu7"
      },
      "source": [
        "### Define the objective function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJY8Z8tuxai7"
      },
      "source": [
        "First we define a custom callback to report the results of periodic evaluations to Optuna:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "U5ijWTPzxSmd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class TrialEvalCallback(EvalCallback):\n",
        "    \"\"\"\n",
        "    Callback used for evaluating and reporting a trial.\n",
        "\n",
        "    :param eval_env: Evaluation environement\n",
        "    :param trial: Optuna trial object\n",
        "    :param n_eval_episodes: Number of evaluation episodes\n",
        "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
        "    :param deterministic: Whether the evaluation should\n",
        "        use a stochastic or deterministic policy.\n",
        "    :param verbose:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_env: gym.Env,\n",
        "        trial: optuna.Trial,\n",
        "        n_eval_episodes: int = 5,\n",
        "        eval_freq: int = 10000,\n",
        "        deterministic: bool = True,\n",
        "        verbose: int = 0,\n",
        "    ):\n",
        "\n",
        "        super().__init__(\n",
        "            eval_env=eval_env,\n",
        "            n_eval_episodes=n_eval_episodes,\n",
        "            eval_freq=eval_freq,\n",
        "            deterministic=deterministic,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        self.trial = trial\n",
        "        self.eval_idx = 0\n",
        "        self.is_pruned = False\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
        "            # Evaluate policy (done in the parent class)\n",
        "            super()._on_step()\n",
        "            self.eval_idx += 1\n",
        "            # Send report to Optuna\n",
        "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
        "            # Prune trial if need\n",
        "            if self.trial.should_prune():\n",
        "                self.is_pruned = True\n",
        "                return False\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cHNM_cFO3vs"
      },
      "source": [
        "### Define the objective function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76voi9AXxlCq"
      },
      "source": [
        "Then we define the objective function that is in charge of sampling hyperparameters, creating the model and then returning the result to Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E0yEokTDxhrC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.Trial) -> float:\n",
        "    \"\"\"\n",
        "    Objective function using by Optuna to evaluate\n",
        "    one configuration (i.e., one set of hyperparameters).\n",
        "\n",
        "    Given a trial object, it will sample hyperparameters,\n",
        "    evaluate it and report the result (mean episodic reward after training)\n",
        "\n",
        "    :param trial: Optuna trial object\n",
        "    :return: Mean episodic reward after training\n",
        "    \"\"\"\n",
        "\n",
        "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
        "\n",
        "    # Sample hyperparameters and update the keyword arguments\n",
        "    kwargs.update(sample_a2c_params(trial))\n",
        "\n",
        "    # Create the RL model\n",
        "    model = A2C(**kwargs)\n",
        "\n",
        "    # Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n",
        "    eval_envs = make_vec_env(ENV_ID,N_EVAL_ENVS)\n",
        "\n",
        "    # Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n",
        "    eval_callback = TrialEvalCallback(eval_envs,trial, N_EVAL_EPISODES, EVAL_FREQ, True, 1)\n",
        "\n",
        "    nan_encountered = False\n",
        "    try:\n",
        "        # Train the model\n",
        "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
        "    except AssertionError as e:\n",
        "        # Sometimes, random hyperparams can generate NaN\n",
        "        print(e)\n",
        "        nan_encountered = True\n",
        "    finally:\n",
        "        # Free memory\n",
        "        model.env.close()\n",
        "        eval_envs.close()\n",
        "\n",
        "    # Tell the optimizer that the trial failed\n",
        "    if nan_encountered:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    if eval_callback.is_pruned:\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return eval_callback.last_mean_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMFLu_M0ymzj"
      },
      "source": [
        "### The optimization loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UU17YpjymPr",
        "vscode": {
          "languageId": "python"
        },
        "outputId": "dda99aea-4e88-45c0-89ad-7f7f5700d01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:12:12,908] A new study created in memory with name: no-name-43efba92-aa7a-4e1f-8520-7f54ab5ddfd2\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=9.40 +/- 0.66\n",
            "Episode length: 9.40 +/- 0.66\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:12:30,827] Trial 0 finished with value: 9.2 and parameters: {'gamma': 0.0001565552546387575, 'max_grad_norm': 1.6446413164628697, 'exponent_n_steps': 8, 'learning_rate': 0.29211762192057805, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 0 with value: 9.2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=9.20 +/- 0.60\n",
            "Episode length: 9.20 +/- 0.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=71.90 +/- 11.85\n",
            "Episode length: 71.90 +/- 11.85\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:12:48,715] Trial 1 finished with value: 87.4 and parameters: {'gamma': 0.00013261331934499, 'max_grad_norm': 0.9773772272117918, 'exponent_n_steps': 3, 'learning_rate': 2.1269700904602606e-05, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 1 with value: 87.4.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=87.40 +/- 44.18\n",
            "Episode length: 87.40 +/- 44.18\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=78.30 +/- 33.35\n",
            "Episode length: 78.30 +/- 33.35\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:13:02,387] Trial 2 finished with value: 92.2 and parameters: {'gamma': 0.05687182639947384, 'max_grad_norm': 0.6586832697701895, 'exponent_n_steps': 4, 'learning_rate': 0.0002542689742822569, 'net_arch': 'tiny', 'activation_fn': 'relu'}. Best is trial 2 with value: 92.2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=92.20 +/- 28.33\n",
            "Episode length: 92.20 +/- 28.33\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=113.70 +/- 33.40\n",
            "Episode length: 113.70 +/- 33.40\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=100.70 +/- 45.97\n",
            "Episode length: 100.70 +/- 45.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:13:14,676] Trial 3 finished with value: 100.7 and parameters: {'gamma': 0.0007340544047573383, 'max_grad_norm': 4.686071443578941, 'exponent_n_steps': 9, 'learning_rate': 0.00019796285091692518, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 3 with value: 100.7.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=488.30 +/- 35.10\n",
            "Episode length: 488.30 +/- 35.10\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:13:27,034] Trial 4 finished with value: 500.0 and parameters: {'gamma': 0.06898337252688375, 'max_grad_norm': 1.0704444392930572, 'exponent_n_steps': 7, 'learning_rate': 0.00889227980581267, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:13:32,960] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=9.40 +/- 0.80\n",
            "Episode length: 9.40 +/- 0.80\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=137.10 +/- 3.36\n",
            "Episode length: 137.10 +/- 3.36\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:13:44,741] Trial 6 finished with value: 237.9 and parameters: {'gamma': 0.00816930731981947, 'max_grad_norm': 2.00779006126141, 'exponent_n_steps': 6, 'learning_rate': 0.006664807592420221, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=237.90 +/- 33.63\n",
            "Episode length: 237.90 +/- 33.63\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:13:49,966] Trial 7 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=9.40 +/- 0.80\n",
            "Episode length: 9.40 +/- 0.80\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:14:01,963] Trial 8 finished with value: 500.0 and parameters: {'gamma': 0.021757294266547465, 'max_grad_norm': 3.112485503184081, 'exponent_n_steps': 7, 'learning_rate': 0.004168826186899289, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:14:08,233] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=9.40 +/- 0.80\n",
            "Episode length: 9.40 +/- 0.80\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:14:13,973] Trial 10 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=93.80 +/- 45.15\n",
            "Episode length: 93.80 +/- 45.15\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=198.20 +/- 29.97\n",
            "Episode length: 198.20 +/- 29.97\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:14:25,606] Trial 11 finished with value: 500.0 and parameters: {'gamma': 0.02566265913506896, 'max_grad_norm': 3.964979309825269, 'exponent_n_steps': 7, 'learning_rate': 0.005423606529239947, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=371.80 +/- 176.54\n",
            "Episode length: 371.80 +/- 176.54\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:14:37,786] Trial 12 finished with value: 410.9 and parameters: {'gamma': 0.024688414958465688, 'max_grad_norm': 2.473566817305961, 'exponent_n_steps': 7, 'learning_rate': 0.0010573695663104482, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=410.90 +/- 136.11\n",
            "Episode length: 410.90 +/- 136.11\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:14:43,269] Trial 13 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=83.90 +/- 11.65\n",
            "Episode length: 83.90 +/- 11.65\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=450.20 +/- 100.89\n",
            "Episode length: 450.20 +/- 100.89\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:14:55,822] Trial 14 finished with value: 198.6 and parameters: {'gamma': 0.008408190333776691, 'max_grad_norm': 1.3706229118733158, 'exponent_n_steps': 5, 'learning_rate': 0.0023098648110028547, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=198.60 +/- 45.30\n",
            "Episode length: 198.60 +/- 45.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:15:01,340] Trial 15 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=22.30 +/- 3.32\n",
            "Episode length: 22.30 +/- 3.32\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:15:06,827] Trial 16 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=100.70 +/- 37.05\n",
            "Episode length: 100.70 +/- 37.05\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:15:13,160] Trial 17 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=9.40 +/- 0.80\n",
            "Episode length: 9.40 +/- 0.80\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=492.80 +/- 21.60\n",
            "Episode length: 492.80 +/- 21.60\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=414.60 +/- 64.65\n",
            "Episode length: 414.60 +/- 64.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:15:25,721] Trial 18 finished with value: 414.6 and parameters: {'gamma': 0.0037346817796266286, 'max_grad_norm': 0.698561380205939, 'exponent_n_steps': 9, 'learning_rate': 0.0018565331094538976, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=215.30 +/- 155.26\n",
            "Episode length: 215.30 +/- 155.26\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:15:37,766] Trial 19 finished with value: 279.6 and parameters: {'gamma': 0.04621051194933854, 'max_grad_norm': 1.39279710940246, 'exponent_n_steps': 6, 'learning_rate': 0.0007372764473025734, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=279.60 +/- 145.34\n",
            "Episode length: 279.60 +/- 145.34\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:15:43,027] Trial 20 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=94.80 +/- 24.55\n",
            "Episode length: 94.80 +/- 24.55\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=320.20 +/- 125.73\n",
            "Episode length: 320.20 +/- 125.73\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:15:54,950] Trial 21 finished with value: 500.0 and parameters: {'gamma': 0.025115024776808346, 'max_grad_norm': 4.610950155397442, 'exponent_n_steps': 7, 'learning_rate': 0.006529135188959501, 'net_arch': 'tiny', 'activation_fn': 'tanh'}. Best is trial 4 with value: 500.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "\n",
        "# Set pytorch num threads to 1 for faster training\n",
        "th.set_num_threads(1)\n",
        "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
        "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
        "# Do not prune before 1/3 of the max budget is used\n",
        "pruner = MedianPruner(\n",
        "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
        ")\n",
        "# Create the study and start the hyperparameter optimization\n",
        "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
        "\n",
        "try:\n",
        "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "print(\"\\n\\n\\n\\n\\nNumber of finished trials: \", len(study.trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Value: {trial.value}\")\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "print(\"  User attrs:\")\n",
        "for key, value in trial.user_attrs.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Write report\n",
        "study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n",
        "\n",
        "fig1 = plot_optimization_history(study)\n",
        "fig2 = plot_param_importances(study)\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yUeYnfJVpB2"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "What we have seen in this notebook:\n",
        "- how to do automatic hyperparameter search with optuna\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}